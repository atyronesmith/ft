# FineTune

A powerful, modular fine-tuning application optimized for Apple Silicon (M4) that enables efficient training of language models from HuggingFace on custom datasets.

## ğŸ¯ Project Status

**âœ… Phase 1 Complete**: Core infrastructure with 106 tests passing
**âœ… Phase 2 COMPLETE**: Production-ready fine-tuning system
- **227 tests passing, 6 skipped** (65% code coverage)
- **100% test pass rate** across all components
- **Test suite fully operational** - Fixed all pytest collection conflicts and dependency mocking
- **Real HuggingFace model integration** - Successfully loads and fine-tunes microsoft/DialoGPT-small
- MLX-native LoRA implementation with 87.5% parameter reduction
- Complete data loading and prompt template system
- Comprehensive configuration management with profiles
- **Production-ready CLI commands** - `ft train quick`, `ft train start`, `ft train validate`
- **Custom MLX weight loading** for transformer architectures
- **Robust testing infrastructure** with proper external dependency mocking

## Features

### âœ… Implemented (Phase 1 & 2 COMPLETE)
- ğŸš€ **Apple Silicon Optimized**: MLX backend with automatic PyTorch fallback
- ğŸ“¦ **Model Management**: HuggingFace downloading, caching, and conversion
- ğŸ”„ **Weight Conversion**: Automatic PyTorch â†’ MLX format conversion with Safetensors priority
- ğŸ—ï¸ **Architecture Support**: Llama, GPT-2, Mistral, DialoGPT models with custom MLX loading
- ğŸ¯ **LoRA Training**: MLX-native LoRA implementation with 87.5% parameter reduction
- ğŸ“Š **Data Pipeline**: JSON/JSONL loading with validation and statistics
- ğŸ¨ **Prompt Templates**: Alpaca, ChatML, Llama + custom template support
- âš™ï¸ **Configuration**: YAML-based config with profiles (chat, instruction, code)
- ğŸ’» **CLI Commands**: Complete `ft train` suite - `quick`, `start`, `validate`
- ğŸ”— **End-to-End Workflow**: Real model fine-tuning from dataset to trained model
- âš¡ **Error Handling**: Comprehensive validation and user-friendly errors
- ğŸ§ª **Testing**: 227 tests passing with 65% coverage, robust dependency mocking, and TDD methodology
- ğŸ”§ **Developer Tools**: Enhanced Makefile, completion scripts, linting
- ğŸ¤– **Real Model Integration**: Successfully loads and trains microsoft/DialoGPT-small

### ğŸš§ In Development (Phase 3)
- ğŸ“Š **Dataset Support**: CSV, Parquet loaders expansion
- ğŸ¨ **Interfaces**: Web UI, REST API
- ğŸ“ˆ **Monitoring**: TensorBoard integration and metrics
- ğŸ”§ **Advanced Features**: Multi-GPU training, quantization options

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/finetune.git
cd finetune

# Run automated setup (recommended)
./setup.sh

# Or manual setup
python3 -m venv .venv
source .venv/bin/activate
pip install -e .

# Verify installation
make test
```

### Basic Usage

```bash
# Activate virtual environment
source .venv/bin/activate

# Quick fine-tuning with real models
ft train quick microsoft/DialoGPT-small examples/sample_dataset.json

# Production training with full configuration
ft train start microsoft/DialoGPT-small data/training.json \
  --template chatml --epochs 5 --batch-size 4 --lora-rank 16 --profile chat

# Validate configuration and estimate memory
ft train validate configs/production.yml

# Development and testing
make test              # All tests with coverage (227 tests)
make test-unit         # Unit tests only
make test-integration  # Integration tests
make test-e2e-quick    # End-to-end validation (recommended)

# Component-specific testing
make test-lora         # LoRA implementation tests
make test-data         # Data loading tests
make test-config       # Configuration tests
make test-e2e-workflow # Workflow integration (mocked)

# Advanced end-to-end testing
FT_REAL_MODEL_ENABLE=1 make test-e2e-real-model  # Real model integration (requires flag)
make test-e2e-mlx                                 # Full MLX pipeline (runs by default)

# Code quality
make lint             # Run all linters
make format          # Format code with black
```

### Real Model Integration Testing

The middle ground integration test loads actual HuggingFace models and validates end-to-end fine-tuning with measurable success criteria:

```bash
# Activate virtual environment
source .venv/bin/activate

# Run real model integration test (basic)
FT_REAL_MODEL_ENABLE=1 make test-e2e-real-model

# Run with verbose step-by-step output (recommended for debugging)
FT_REAL_MODEL_ENABLE=1 FT_VERBOSE=1 make test-e2e-real-model

# Or run directly with pytest to see all output
FT_REAL_MODEL_ENABLE=1 FT_VERBOSE=1 pytest tests/integration/test_end_to_end_real_model.py -v -s
```

#### What the Test Does:
- **Loads real models**: Downloads and converts HuggingFace models (DistilGPT2, microsoft/DialoGPT-small)
- **Resource optimization**: Automatically selects optimal config based on available memory
- **Deterministic training**: Uses structured Q&A dataset with geography, math, and pattern completion
- **Measurable validation**: Tests 4 objective success criteria:
  1. **Loss convergence**: Training loss decreases meaningfully
  2. **Model learning**: Improved responses on test questions
  3. **Memory efficiency**: LoRA provides 30%+ memory savings
  4. **Training artifacts**: Output files and logs are created

#### Verbose Output Features:
- ğŸ¯ **Configuration optimization** with memory detection
- ğŸ“Š **Dataset generation** with category breakdown
- ğŸš€ **Model loading progress** with parameter counts
- ğŸ”„ **Training execution** with loss tracking
- âœ… **Success validation** with detailed pass/fail status
- ğŸ‰ **Comprehensive summary** with efficiency metrics

#### Environment Variables:
- `FT_REAL_MODEL_ENABLE=1` - Enable real model testing (required)
- `FT_VERBOSE=1` - Enable detailed step-by-step output
- `FT_TEST_MODEL` - Override test model (default: microsoft/DialoGPT-small)

## Project Structure

```
ft/
â”œâ”€â”€ src/finetune/         # Main package source code
â”‚   â”œâ”€â”€ models/          # Model implementations (MLX, PyTorch)
â”‚   â”œâ”€â”€ backends/        # Backend abstraction layer
â”‚   â”œâ”€â”€ cli/            # CLI commands (in development)
â”‚   â”œâ”€â”€ core/           # Core utilities and config
â”‚   â””â”€â”€ training/       # Training loops (Phase 2)
â”œâ”€â”€ tests/               # Test suite
â”‚   â”œâ”€â”€ unit/           # Unit tests (66 passing)
â”‚   â””â”€â”€ integration/    # Integration tests
â”œâ”€â”€ docs/               # Documentation
â”‚   â””â”€â”€ design/        # Architecture documents
â”œâ”€â”€ scripts/            # Utility scripts
â””â”€â”€ .pre-commit-config.yaml  # Code quality hooks
```

## Current Implementation Details

### Model Loading

```python
from finetune.models.manager import ModelManager

# Initialize manager
manager = ModelManager()

# List available models
models = manager.list_models()

# Load a model (automatic backend selection)
model = manager.load_model("meta-llama/Llama-2-7b-hf")

# Estimate memory usage
memory = manager.estimate_memory_usage(
    "model-name",
    batch_size=4,
    sequence_length=2048,
    training=True
)
```

### Backend Support

The framework automatically selects the optimal backend:

1. **MLX** (Apple Silicon): Native performance on M1/M2/M3/M4
2. **PyTorch MPS**: Metal Performance Shaders fallback
3. **PyTorch CUDA**: NVIDIA GPU support
4. **PyTorch CPU**: Universal fallback

### Supported Models

Currently implemented architectures:
- **Llama Family**: Llama, Llama2, Llama3
- **GPT Family**: GPT-2, GPT-J, GPT-Neo
- **Mistral**: All Mistral variants

## Development

### Setup Development Environment

```bash
# Install with dev dependencies
./setup.sh

# Or manually
python3 -m venv .venv
source .venv/bin/activate
pip install -e .
pip install pytest pytest-mock pytest-cov
pip install black ruff isort mypy pylint
pre-commit install
```

### Running Tests

```bash
# Run all unit tests
make test

# Run specific test file
pytest tests/unit/test_mlx_models.py -v

# Run with coverage
pytest --cov=finetune tests/

# Run specific test
pytest tests/unit/test_mlx_models.py::TestMLXModels::test_llama_model_creation
```

### Code Quality

```bash
# Format code
make format          # Runs black

# Lint code
make lint           # Runs ruff, pylint, mypy

# Pre-commit hooks (automatic)
git commit          # Triggers pre-commit hooks

# Manual pre-commit
pre-commit run --all-files
```

## Test Coverage Report

**Phase 1 Complete**: **106 total tests** with **100% pass rate**
- Unit Tests: 87 passing, 2 skipped
- Integration Tests: 16 passing, 1 skipped
- Code Coverage: **61%** on core modules

```
Name                                     Stmts   Miss  Cover
--------------------------------------------------------------
src/finetune/models/base.py                67      7    90%
src/finetune/models/mlx_loader.py         138     23    83%
src/finetune/models/mlx_models.py         226     26    88%
src/finetune/models/torch_loader.py        98      6    94%
src/finetune/models/manager.py             93     40    57%
src/finetune/backends/device.py            92     42    54%
src/finetune/core/config.py               116     55    53%
--------------------------------------------------------------
TOTAL                                     1348    634    53%
```

## System Requirements

- **macOS**: 12.0+ (Monterey or later) for MLX support
- **Hardware**: Apple Silicon (M1/M2/M3/M4) recommended
- **Python**: 3.11+ required
- **Memory**: 16GB+ RAM recommended
- **Storage**: 10GB+ for models and cache

## Performance Benchmarks

On Apple M-series chips (via MLX):
- **Model Loading**: 2-3x faster than PyTorch
- **Memory Usage**: 40% less due to unified memory
- **Weight Conversion**: ~5 seconds for 7B parameter models

## Troubleshooting

### Common Issues

**Import Errors**
```bash
# Ensure virtual environment is activated
source .venv/bin/activate
# Reinstall dependencies
pip install -e .
```

**MLX Not Available**
```bash
# Check if on Apple Silicon
python -c "import platform; print(platform.machine())"
# Should output: arm64

# Install MLX
pip install mlx
```

**Test Failures**
```bash
# Clear pytest cache
pytest --cache-clear
# Run with verbose output
pytest -xvs tests/unit/
```

## Contributing

We welcome contributions! The codebase uses:
- Type hints throughout (enforced by mypy)
- Black formatting (line length 100)
- Comprehensive docstrings
- Pre-commit hooks for quality

Before submitting:
1. Run tests: `make test`
2. Run linters: `make lint`
3. Format code: `make format`

## Roadmap

### Phase 1 âœ… (Complete)
- [x] Project structure and build system
- [x] MLX and PyTorch backend abstraction
- [x] Model loading from HuggingFace
- [x] Weight conversion system
- [x] Test suite and CI/CD

### Phase 2 âœ… (Complete)
- [x] Data loading pipelines with JSON/JSONL support
- [x] Training loops with LoRA implementation
- [x] CLI commands implementation (`ft train` suite)
- [x] Configuration system with YAML and profiles
- [x] Real HuggingFace model integration
- [x] End-to-end workflow implementation

### Phase 3 ğŸ“‹ (Planned)
- [ ] Web UI with Streamlit
- [ ] REST API with FastAPI
- [ ] Distributed training
- [ ] Model quantization

### Phase 4 ğŸ”® (Future)
- [ ] Custom model architectures
- [ ] Advanced optimization techniques
- [ ] Cloud deployment options
- [ ] Model marketplace integration

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

Built with:
- [MLX](https://github.com/ml-explore/mlx) - Apple's ML framework
- [Transformers](https://github.com/huggingface/transformers) - HuggingFace's model library
- [PyTorch](https://pytorch.org) - Fallback backend
- [PEFT](https://github.com/huggingface/peft) - Parameter-efficient fine-tuning (planned)

## Support

- Issues: [GitHub Issues](https://github.com/yourusername/finetune/issues)
- Discussions: [GitHub Discussions](https://github.com/yourusername/finetune/discussions)
- Documentation: See `/docs` folder for architecture and design docs
