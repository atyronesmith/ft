# Project Status (CI-Generated Source of Truth)

Last Updated: 2025-09-21

Note: This file is intended to be generated by CI. Until automation is wired, values are manually maintained.

## Versions

- Version: 0.2.1-dev
- Phase: **CRITICAL ISSUE - Training System Broken**
- Test Suite: E2E Training Failing

## Current Critical Issue: MLX LoRA Training System Failure

### Problem Analysis (2025-09-21)

**Symptom:** End-to-end training tests fail with "AttributeError: 'dict' object has no attribute 'forward'" and related MLX value_and_grad errors.

**Root Cause:** Fundamental architectural mismatch between PyTorch-style LoRA implementation and MLX automatic differentiation system.

**Technical Details:**
1. **MLX `value_and_grad` behavior** - Converts model objects to parameter trees (dictionaries), breaking model's `forward()` method
2. **Parameter navigation complexity** - MLX models use nested structures (lists for layers) incompatible with path-based parameter updates
3. **LoRA parameter isolation** - Current implementation expects PyTorch-style parameter handling incompatible with MLX's tree-based approach
4. **Training loop integration** - Attempts to bridge PyTorch patterns with MLX automatic differentiation fail at runtime

**Issues Successfully Resolved:**
- ✅ MLX sampling errors (`mx.random.categorical()` int conversion)
- ✅ Template consistency (centralized utilities in `src/finetune/utils/chat.py`)
- ✅ Generation tokenization problems
- ✅ Cross-machine environment compatibility (Poetry detection + pip fallback)

**Current Blocker:** Training system architecture requires complete redesign to work natively with MLX parameter trees.

## Test Summary

- **Total tests: ~233 collected**
- **Unit tests: 227 passing**
- **Integration tests: E2E training FAILING**
- **Code coverage: ~36%** (reduced due to training failures)
- **Critical path: Broken** (cannot complete fine-tuning workflow)

## Phase Status

- **Phase 1: ✅ Complete** (Core infrastructure)
- **Phase 2: ❌ REGRESSION** (LoRA training system broken)
- **Immediate Priority: Fix MLX training integration**
- Phase 3: Blocked pending Phase 2 fix
- Phase 4: Blocked pending Phase 2 fix

## Detailed Fix Plan

### Priority 1: MLX LoRA Training Architecture Redesign

#### Option A: Native MLX Pattern (Recommended)
**Timeline: 2-4 hours**
**Approach:** Rewrite training loop to use MLX's native parameter tree approach

**Implementation Plan:**
1. **Study MLX LoRA Examples** (30 mins)
   - Research `mlx-examples` repository for working LoRA implementations
   - Identify correct MLX value_and_grad patterns
   - Document proper parameter tree handling

2. **Redesign Training Step** (2 hours)
   - Replace current `training_step()` with native MLX approach
   - Use `tree_map()` for parameter filtering instead of path-based navigation
   - Implement proper parameter tree updates compatible with MLX optimizer
   - Example pattern:
   ```python
   # Native MLX approach
   def loss_fn(model):
       return compute_loss(model, batch)

   # Get gradients for entire model
   loss, grads = mx.value_and_grad(loss_fn)(model)

   # Filter to only LoRA parameters using tree operations
   def is_lora_param(path, param):
       return 'lora_a' in path or 'lora_b' in path

   lora_grads = tree_filter(is_lora_param, grads)

   # Update using MLX optimizer with tree
   optimizer.update(model, lora_grads)
   ```

3. **Test and Validate** (1 hour)
   - Run `make test-e2e-mlx-short` to verify training completes
   - Validate loss convergence and parameter updates
   - Confirm model generates proper geography answers

4. **Integration Testing** (30 mins)
   - Test with longer training durations
   - Verify model quality and generation consistency

#### Option B: Reference Implementation (Fallback)
**Timeline: 4-6 hours**
**Approach:** Replace current LoRA implementation with proven MLX LoRA code

**Implementation Plan:**
1. **Source Reference Implementation** (1 hour)
   - Find working MLX LoRA implementation (mlx-examples, community repos)
   - Review license compatibility and integration requirements

2. **Integration** (3-4 hours)
   - Replace `src/finetune/training/lora.py` with reference implementation
   - Update trainer to use reference LoRA patterns
   - Modify model loading to work with reference approach

3. **Testing and Debugging** (1-2 hours)
   - Fix integration issues
   - Validate end-to-end workflow

### Priority 2: Comprehensive Testing
**Timeline: 1 hour**

1. **Restore E2E Test Coverage** (30 mins)
   - Ensure all training tests pass
   - Validate model quality improvements

2. **Performance Validation** (30 mins)
   - Confirm training speed and memory usage acceptable
   - Test with different model sizes and LoRA configurations

### Priority 3: Documentation and Monitoring
**Timeline: 30 mins**

1. **Update Status** (15 mins)
   - Mark Phase 2 as ✅ Complete once training works
   - Update test summary with passing counts

2. **Add Training Monitoring** (15 mins)
   - Improve logging for training step debugging
   - Add parameter update validation

## Success Criteria

**Phase 2 Restoration Complete When:**
- ✅ `make test-e2e-mlx-short` passes consistently
- ✅ Model generates proper geography answers (not gibberish)
- ✅ Training loss converges properly
- ✅ No AttributeError or MLX value_and_grad exceptions
- ✅ All unit tests continue passing (227+)

## Links

- Repository: https://github.com/yourusername/finetune
- Critical Issue: MLX LoRA training architecture mismatch
- Coverage report: htmlcov/index.html
- Training Test: `make test-e2e-mlx-short`
