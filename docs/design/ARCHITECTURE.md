# Fine-Tuning Application Architecture

**Last Updated**: September 2025
**Status**: âœ… Phase 1 Complete (106 tests passing), ðŸš§ Phase 2 In Progress (LoRA Implementation)

## System Overview
A modular, extensible fine-tuning platform leveraging Apple Silicon optimization for efficient local model training.

## Core Architecture Components

### 1. Model Layer âœ… IMPLEMENTED
- **Model Manager**: Unified interface for all model operations with automatic backend selection
- **MLX Native Models**: Direct MLX implementations (Llama, Mistral, GPT-2) with optimal Apple Silicon performance
- **PyTorch Fallback**: Seamless fallback to PyTorch MPS/CUDA when MLX unavailable
- **Weight Conversion**: Automated PyTorch â†’ MLX conversion pipeline with validation
- **Model Loading**: Support for sharded models, safetensors, and HuggingFace Hub integration

### 2. Data Pipeline ðŸš§ PHASE 2
- **Format Handlers**: Pluggable parsers for JSON, JSONL, CSV, Parquet, and custom formats
- **Template Engine**: Configurable prompt templates (Alpaca, ChatML, Llama, custom)
- **Preprocessing Pipeline**: Token analysis, sequence length optimization, data validation
- **Dataset Cache**: LRU cache with memory-mapped storage for efficient data loading

### 3. Training Infrastructure ðŸš§ PHASE 2
- **Backend Selection**: Intelligent MLX/PyTorch selection with device detection (âœ… COMPLETE)
- **Memory Management**: Unified memory monitoring and automatic batch sizing (âœ… COMPLETE)
- **MLX Backend**: Primary training backend optimized for Apple Silicon (âœ… COMPLETE)
- **PyTorch Fallback**: Secondary backend with MPS acceleration (âœ… COMPLETE)
- **Training Loops**: LoRA/QLoRA implementations with checkpointing (ðŸš§ IN PROGRESS)

### 4. Configuration Management ðŸš§ PHASE 2
- **Primary Config**: Single `train.yml` file for all training parameters
- **Secrets Management**: Separate `passwords.yml` for API keys and tokens
- **Profile System**: Predefined profiles for common scenarios (chat, instruction, code, domain-specific)
- **Hyperparameter Optimizer**: Bayesian optimization for automatic hyperparameter tuning

## Technical Stack

### Core Technologies
- Python 3.11+ (primary language)
- MLX (Apple's ML framework) for M4 optimization
- Transformers library for model handling
- FastAPI for API server
- Typer for CLI interface

### Storage & Persistence
- SQLite for metadata and job tracking
- HDF5 for efficient dataset storage
- Git-based version control for configurations

## Application Workflow

```
1. Model Selection
   â”œâ”€â”€ Browse HuggingFace models
   â”œâ”€â”€ Filter by size/compatibility
   â””â”€â”€ Download and cache locally

2. Dataset Preparation
   â”œâ”€â”€ Load from file/URL
   â”œâ”€â”€ Apply formatting template
   â”œâ”€â”€ Validate and analyze
   â””â”€â”€ Split train/validation

3. Configuration
   â”œâ”€â”€ Edit train.yml
   â”œâ”€â”€ Set secrets in passwords.yml
   â”œâ”€â”€ Choose optimization method
   â””â”€â”€ Set resource limits

4. Training Execution
   â”œâ”€â”€ Initialize model with adapters
   â”œâ”€â”€ Load data batches
   â”œâ”€â”€ Execute training loop
   â”œâ”€â”€ Monitor metrics
   â””â”€â”€ Save checkpoints

5. Evaluation & Export
   â”œâ”€â”€ Run validation metrics
   â”œâ”€â”€ Generate sample outputs
   â”œâ”€â”€ Export to various formats
   â””â”€â”€ Push to hub (optional)
```

## Data Pipeline Architecture

### Input Formats Support
- **Structured**: JSON, JSONL, CSV, TSV, Parquet
- **Unstructured**: Plain text, Markdown, HTML (with extraction)
- **Specialized**: ShareGPT, Alpaca, OpenAI chat format

### Processing Stages
1. **Ingestion**: Multi-threaded file readers with streaming support
2. **Validation**: Schema validation, missing value handling, format consistency
3. **Transformation**: Template application, tokenization preview, sequence chunking
4. **Augmentation**: Optional synthetic data generation, paraphrasing, back-translation
5. **Indexing**: Create efficient access patterns for training

### Data Quality Features
- Automatic deduplication
- Length distribution analysis
- Token complexity metrics
- Class balance reporting
- Contamination detection (training/test overlap)

## Model Management & Training Infrastructure

### Model Capabilities
- **Supported Architectures**: Llama, Mistral, Phi, Qwen, Gemma, GPT-2/J, BERT variants
- **Size Range**: 0.5B to 70B parameters (with quantization)
- **Training Methods**: 
  - Full fine-tuning (small models)
  - LoRA/QLoRA (recommended for M4)
  - Prefix tuning
  - P-tuning v2

### Training Optimization for M4
- **Memory Management**:
  - Unified memory pool monitoring
  - Gradient checkpointing
  - Dynamic batch size adjustment
  - Activation offloading
- **Compute Optimization**:
  - Mixed precision training (FP16/BF16)
  - Fused kernels for common operations
  - Neural Engine utilization where possible
  - Multi-threaded data loading

### Training Features
- Real-time loss visualization
- Learning rate scheduling (cosine, linear, warmup)
- Early stopping with patience
- Gradient accumulation for larger effective batches
- Distributed data parallel (for multi-GPU future support)

### Checkpoint Management
- Incremental checkpointing
- Best model tracking
- Resume from interruption
- Checkpoint merging for LoRA adapters

## User Interface Design

### CLI Interface (Primary)
```bash
# Main commands structure
ft init                           # Initialize project
ft models list                     # Browse available models
ft models pull llama-3.2-3b       # Download model
ft dataset prepare data.json      # Prepare dataset
ft train                           # Start training with train.yml
ft evaluate --checkpoint best      # Run evaluation
ft export --format gguf           # Export model
ft serve --port 8000              # Launch inference server
```

### Web Dashboard (Secondary)
- **Dashboard**: Training metrics, resource usage, job queue
- **Model Browser**: Search, filter, and preview models
- **Dataset Editor**: Upload, preview, and edit datasets
- **Configuration Builder**: Visual hyperparameter tuning
- **Training Monitor**: Real-time loss graphs, sample outputs
- **Export Manager**: Format conversion, quantization options

### API Endpoints
```
POST /api/training/start
GET  /api/training/{job_id}/status
GET  /api/models/search
POST /api/datasets/upload
GET  /api/metrics/{job_id}
POST /api/inference/generate
```

## Deployment & Packaging Strategy

### Distribution Methods

1. **Homebrew Formula**:
   - Simple installation: `brew install finetune`
   - Automatic dependency management
   - Easy updates

2. **Python Package**:
   - PyPI distribution: `pip install finetune-m4`
   - Virtual environment support
   - Dependency isolation

3. **Standalone App**:
   - DMG installer with code signing
   - Self-contained Python runtime
   - Automatic updates via Sparkle framework

### Directory Structure
```
~/Library/Application Support/FineTune/
â”œâ”€â”€ models/          # Downloaded models
â”œâ”€â”€ datasets/        # Cached datasets  
â”œâ”€â”€ checkpoints/     # Training checkpoints
â”œâ”€â”€ configs/         # User configurations
â””â”€â”€ logs/           # Training logs
```

### Performance Considerations
- Model sharding for >32GB models
- Memory-mapped weight loading
- Lazy loading for large datasets
- Background model downloading
- Automatic garbage collection for old checkpoints

### Security & Privacy
- Local-only processing (no telemetry)
- Encrypted passwords.yml support
- Sandboxed file system access
- Code signing for macOS Gatekeeper

## Project Development Roadmap

### âœ… Phase 1: Foundation (COMPLETE)
- âœ… Core project structure (66 unit tests, 40 integration tests)
- âœ… Full MLX integration with backend abstraction
- âœ… Model loading for Llama, Mistral, GPT-2 architectures
- âœ… Weight conversion pipeline (PyTorch â†’ MLX)
- âœ… Memory management and device detection
- âœ… Comprehensive test suite (106 tests passing)

### ðŸš§ Phase 2: Training Pipeline (IN PROGRESS)
- ðŸš§ LoRA/QLoRA implementation for MLX
- ðŸš§ Training loop orchestration
- ðŸš§ Dataset loaders and preprocessing
- ðŸš§ Configuration system with train.yml

### Phase 3: Optimization (Weeks 5-6)
- Memory optimization
- Quantization support
- Performance profiling
- Checkpoint management

### Phase 4: Interface (Weeks 7-8)
- Complete CLI
- Web dashboard
- API server
- Documentation

### Phase 5: Polish (Weeks 9-10)
- Testing suite
- Packaging
- Distribution setup
- Example notebooks

## Configuration Strategy

### Primary Configuration (train.yml)
All training parameters in a single top-level file:
- Model selection and settings
- Dataset paths and preprocessing
- Training hyperparameters
- Resource limits and optimization
- Output and logging preferences

### Secrets Management (passwords.yml)
Separate file for sensitive information:
- HuggingFace API token
- Weights & Biases API key
- Other service credentials
- Database passwords
- JWT secrets

## Key Design Decisions

1. **MLX-First Approach**: Leverage Apple's framework for optimal M4 performance
2. **Modular Architecture**: Plugin system for datasets, models, and training methods
3. **Simple Configuration**: Single train.yml for all parameters, separate passwords.yml for secrets
4. **Progressive Disclosure**: Simple defaults with advanced options available
5. **Offline-First**: Full functionality without internet after initial model download
6. **Resource-Aware**: Automatic adaptation to available memory and compute

This architecture provides a robust foundation for a fine-tuning application that can handle everything from small BERT models to large LLMs, optimized specifically for Apple Silicon acceleration.