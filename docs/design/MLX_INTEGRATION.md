# MLX Integration Plan for Phase 1

**Last Updated**: September 2025
**Status**: âœ… Phase 1 Complete - All Core Components Implemented (106 tests passing)

## Overview
MLX is Apple's machine learning framework optimized for Apple Silicon. This document outlines the integration steps needed for Phase 1 foundation.

## Core Components

### 1. Device Management (`src/finetune/backends/device.py`)
```python
class DeviceManager:
    - detect_hardware() -> DeviceInfo
    - get_available_memory() -> int
    - select_backend() -> Backend
    - monitor_memory_usage() -> MemoryStats
```

Key tasks:
- Detect M-series chip (M1/M2/M3/M4)
- Query unified memory availability
- Check MLX installation
- Fallback logic to PyTorch MPS

### 2. Backend Interface (`src/finetune/backends/base.py`)
```python
class Backend(ABC):
    - load_model(model_name: str) -> Model
    - create_optimizer(params, lr: float) -> Optimizer
    - compute_loss(logits, labels) -> Tensor
    - backward(loss) -> None
    - step() -> None
    - to_device(tensor) -> Tensor
    - save_checkpoint(path: str) -> None
    - load_checkpoint(path: str) -> None
```

### 3. MLX Backend (`src/finetune/backends/mlx_backend.py`)
```python
class MLXBackend(Backend):
    - convert_weights_from_torch(state_dict) -> mlx.core.array
    - quantize_model(bits: int) -> None
    - compile_model() -> None
    - enable_gradient_checkpointing() -> None
```

Key MLX-specific features:
- Use `mlx.core` for tensor operations
- `mlx.nn` for neural network layers
- `mlx.optimizers` for training
- `mlx.utils` for checkpointing

### 4. Model Loader (`src/finetune/models/mlx_loader.py`)
```python
class MLXModelLoader:
    - load_from_huggingface(model_id: str) -> MLXModel
    - convert_torch_to_mlx(torch_model) -> MLXModel
    - apply_lora_adapters(model, config) -> MLXModel
    - quantize(model, bits: int) -> MLXModel
```

Conversion process:
1. Download PyTorch weights from HuggingFace
2. Convert to MLX arrays using `mlx.core.array()`
3. Reconstruct model architecture in MLX
4. Verify conversion with sample forward pass

### 5. Training Loop (`src/finetune/training/mlx_trainer.py`)
```python
class MLXTrainer:
    - train_step(batch) -> loss
    - validation_step(batch) -> metrics
    - gradient_accumulation_step() -> None
    - mixed_precision_training() -> None
```

MLX training features:
- Automatic differentiation with `mlx.grad()`
- Memory-efficient operations
- Lazy evaluation for optimization
- Graph compilation for performance

## Implementation Steps

### Step 1: Setup and Dependencies
```bash
pip install mlx>=0.10.0
pip install mlx-lm  # For language model utilities
```

### Step 2: Basic Model Loading
```python
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM

def load_and_convert_model(model_name: str):
    # Load PyTorch model
    torch_model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # Convert to MLX
    mlx_weights = {}
    for name, param in torch_model.named_parameters():
        mlx_weights[name] = mx.array(param.detach().numpy())
    
    return mlx_weights
```

### Step 3: Simple Training Loop
```python
import mlx.core as mx
import mlx.optimizers as optim

def train_step(model, batch, optimizer):
    def loss_fn(model, batch):
        logits = model(batch['input_ids'])
        loss = mx.mean(
            nn.losses.cross_entropy(
                logits.reshape(-1, logits.shape[-1]),
                batch['labels'].reshape(-1)
            )
        )
        return loss
    
    # Compute gradients
    loss, grads = mx.value_and_grad(loss_fn)(model, batch)
    
    # Update weights
    optimizer.update(model, grads)
    
    return loss
```

### Step 4: Memory Management
```python
import mlx.core as mx
import psutil

class MemoryManager:
    def __init__(self, reserved_gb: int = 8):
        self.reserved_bytes = reserved_gb * 1024**3
        
    def get_available_memory(self):
        total = psutil.virtual_memory().total
        used = psutil.virtual_memory().used
        available = total - used - self.reserved_bytes
        return max(0, available)
    
    def adjust_batch_size(self, model_size: int):
        available = self.get_available_memory()
        # Estimate batch size based on model and available memory
        estimated_batch = available // (model_size * 4)  # 4x for gradients
        return max(1, estimated_batch)
```

### Step 5: Fallback Mechanism
```python
class BackendSelector:
    @staticmethod
    def get_backend():
        try:
            import mlx.core as mx
            # Test MLX availability
            test = mx.array([1, 2, 3])
            return MLXBackend()
        except ImportError:
            print("MLX not available, falling back to PyTorch")
            return PyTorchBackend()
```

## Testing Requirements

### âœ… Unit Tests (Completed - 66 passing)
- âœ… Test weight conversion accuracy (15 tests)
- âœ… Verify forward pass outputs match PyTorch (12 tests)
- âœ… Test gradient computation (8 tests)
- âœ… Memory usage tracking (6 tests)
- âœ… Model initialization and configuration (15 tests)
- âœ… Backend selection and device detection (10 tests)

### âœ… Integration Tests (Completed - 40 passing)
- âœ… Load various model architectures (Llama, Mistral, GPT-2)
- âœ… Weight conversion pipeline end-to-end
- âœ… Fallback mechanism testing (MLX â†’ PyTorch)
- âœ… HuggingFace model loading and caching
- âœ… Memory management and monitoring
- ðŸš§ Train for a few steps and verify loss decreases (Phase 2)

### Performance Benchmarks
- Compare training speed MLX vs PyTorch MPS
- Memory usage comparison
- Throughput (tokens/second)
- Power efficiency monitoring

## Known Limitations & Solutions

### Current MLX Limitations:
1. **Limited operator support**: Some PyTorch ops may not have MLX equivalents
   - Solution: Implement custom ops or fallback to PyTorch for specific layers

2. **Model architecture constraints**: Not all architectures fully supported
   - Solution: Start with well-supported models (Llama, Mistral)

3. **Quantization differences**: MLX quantization may differ from bitsandbytes
   - Solution: Implement custom quantization schemes

4. **Documentation gaps**: MLX is newer with less documentation
   - Solution: Refer to source code and Apple's examples

## Success Criteria for Phase 1

### âœ… Completed Items
âœ“ Successfully load models (GPT-2, Llama, Mistral) in MLX
âœ“ Run forward pass and get logits
âœ“ Compute loss and gradients  
âœ“ Update weights with optimizer
âœ“ Save and load checkpoints
âœ“ Automatic fallback to PyTorch when MLX unavailable
âœ“ Basic memory monitoring and management
âœ“ Weight conversion from PyTorch to MLX
âœ“ Support for sharded and safetensors formats
âœ“ Comprehensive test suite (69 tests passing)
âœ“ Proper test isolation for MLX availability

### Test Coverage Summary
| Component | Tests | Status |
|-----------|-------|---------|
| MLX Models | 25 | âœ… All passing |
| MLX Loader | 15 | âœ… All passing |
| PyTorch Loader | 19 | âœ… All passing |
| Model Manager | 12 | âœ… All passing |
| Backend Device | 8 | âœ… All passing |
| Model Base | 10 | âœ… All passing |
| Core Registry | 6 | âœ… All passing |
| Integration | 11 | âœ… All passing |
| **Total** | **106** | **âœ… 100% passing** |

## Next Steps (Phase 2)
- LoRA/QLoRA implementation in MLX
- Advanced memory optimization techniques
- Multi-device training support
- Custom CUDA kernel equivalents in Metal